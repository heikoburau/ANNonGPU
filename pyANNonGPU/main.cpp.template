#define __PYTHONCC__

#ifndef LEAN_AND_MEAN
#include "network_functions/ExpectationValue.hpp"
#include "network_functions/HilbertSpaceDistance.hpp"
#include "network_functions/KullbackLeibler.hpp"
#include "network_functions/TDVP.hpp"
#endif // LEAN_AND_MEAN

#include "network_functions/PsiVector.hpp"
#include "network_functions/PsiNorm.hpp"
#include "network_functions/PsiOkVector.hpp"
#include "network_functions/ApplyOperator.hpp"
#include "quantum_states.hpp"
#include "quantum_state/psi_functions.hpp"
#include "ensembles.hpp"
#include "operators.hpp"
#include "bases.hpp"
#include "RNGStates.hpp"
#include "types.h"

#include <pybind11/pybind11.h>
#include <pybind11/operators.h>
#include <pybind11/stl.h>

#define FORCE_IMPORT_ARRAY
#include "xtensor/xadapt.hpp"
#include <xtensor-python/pytensor.hpp>

#include <iostream>
#include <complex>


namespace py = pybind11;

using namespace ann_on_gpu;
using namespace pybind11::literals;

template<unsigned int dim, typename real_dtype = double>
using complex_tensor = xt::pytensor<std::complex<real_dtype>, dim>;

template<unsigned int dim, typename dtype = double>
using real_tensor = xt::pytensor<dtype, dim>;


template<typename dtype, unsigned int dim>
struct std_tensor_t;

template<unsigned int dim>
struct std_tensor_t<double, dim> {
    using type = xt::pytensor<double, dim>;
};

template<unsigned int dim>
struct std_tensor_t<complex_t, dim> {
    using type = xt::pytensor<std::complex<double>, dim>;
};

template<typename dtype, unsigned int dim>
using std_tensor = typename std_tensor_t<dtype, dim>::type;


// Python Module and Docstrings

PYBIND11_MODULE(_pyANNonGPU, m)
{
    xt::import_numpy();

    #ifdef ENABLE_PSI_DEEP

    py::class_<PsiDeep>(m, "PsiDeep")
        .def(py::init<
            const unsigned int,
            const std_tensor<PsiDeep::dtype, 1u>&,
            const vector<std_tensor<PsiDeep::dtype, 1u>>&,
            const vector<xt::pytensor<unsigned int, 2u>>&,
            const vector<std_tensor<PsiDeep::dtype, 2u>>&,
            const std_tensor<PsiDeep::dtype, 1u>&,
            const double,
            const bool
        >())
        .def("copy", &PsiDeep::copy)
        .def_readwrite("num_sites", &PsiDeep::num_sites)
        .def_property(
            "log_prefactor",
            [](const PsiDeep& psi) {return cuda_complex::to_std(psi.log_prefactor);},
            [](PsiDeep& psi, typename std_dtype<PsiDeep::dtype>::type value) {psi.log_prefactor = PsiDeep::dtype(value);}
        )
        .def_readwrite("N_i", &PsiDeep::N_i)
        .def_readwrite("N_j", &PsiDeep::N_j)
        .def_readonly("gpu", &PsiDeep::gpu)
        .def_readonly("N", &PsiDeep::N)
        .def_readonly("num_params", &PsiDeep::num_params)
        .def_property(
            "params",
            [](const PsiDeep& psi) {return psi.get_params().to_pytensor_1d();},
            [](PsiDeep& psi, const std_tensor<PsiDeep::dtype, 1u> new_params) { // reference is to pytensor leads to undefined behavior
                psi.set_params(Array<PsiDeep::dtype>(new_params, false));
            }
        )
        .def_property_readonly("symmetric", [](const PsiDeep& psi){return psi.is_symmetric();})
        .def_property_readonly("a", [](const PsiDeep& psi) {return psi.input_weights.to_pytensor_1d();})
        .def_property_readonly("b", &PsiDeep::get_b)
        .def_property_readonly("connections", &PsiDeep::get_connections)
        .def_property_readonly("W", &PsiDeep::get_W)
        .def_property_readonly("input_weights", [](const PsiDeep& psi) {return psi.input_weights.to_pytensor_1d();})
        .def_property_readonly("final_weights", [](const PsiDeep& psi) {return psi.final_weights.to_pytensor_1d();})
        #ifdef ENABLE_EXACT_SUMMATION
        //! begin template
        .def("_vector", [](PsiDeep& psi, ExactSummation_t<Basis>& exact_summation) {return psi_vector(psi, exact_summation).to_pytensor_1d();})
        .def("norm", [](PsiDeep& psi, ExactSummation_t<Basis>& exact_summation) {return psi_norm(psi, exact_summation);})
        //! end template
        #endif // ENABLE_EXACT_SUMMATION
        ;

    #endif // ENABLE_PSI_DEEP

    #ifdef ENABLE_PSI_CNN

    py::class_<PsiCNN>(m, "PsiCNN")
        .def(py::init<
            const unsigned int,
            const unsigned int,
            const xt::pytensor<unsigned int, 1u>&,
            const xt::pytensor<unsigned int, 1u>&,
            const std_tensor<PsiCNN::dtype, 1u>&,
            const PsiCNN::std_dtype&,
            const double,
            const bool
        >())
        .def("copy", &PsiCNN::copy)
        .def("init_gradient", &PsiCNN::init_gradient)
        .def_readonly("N", &PsiCNN::N)
        .def_readonly("num_sites", &PsiCNN::num_sites)
        .def_property_readonly("num_channels_list", [](const PsiCNN& psi) {return psi.num_channels_list.to_pytensor_1d();})
        .def_property_readonly("connectivity_list", [](const PsiCNN& psi) {return psi.connectivity_list.to_pytensor_1d();})
        .def_readwrite("final_factor", &PsiCNN::final_factor)
        .def_property(
            "log_prefactor",
            [](const PsiCNN& psi) {return cuda_complex::to_std(psi.log_prefactor);},
            [](PsiCNN& psi, PsiCNN::std_dtype value) {psi.log_prefactor = PsiCNN::dtype(value);}
        )
        .def_readonly("gpu", &PsiCNN::gpu)
        .def_readonly("num_params", &PsiCNN::num_params)
        .def_property(
            "params",
            [](const PsiCNN& psi) {return psi.params.to_pytensor_1d();},
            [](PsiCNN& psi, const std_tensor<PsiCNN::dtype, 1u>& new_params) {
                psi.params = new_params;
            }
        )
        #ifdef ENABLE_EXACT_SUMMATION
        //! begin template
        .def("_vector", [](PsiCNN& psi, ExactSummation_t<Basis>& exact_summation) {return psi_vector(psi, exact_summation).to_pytensor_1d();})
        .def("norm", [](PsiCNN& psi, ExactSummation_t<Basis>& exact_summation) {return psi_norm(psi, exact_summation);})
        //! end template
        #endif // ENABLE_EXACT_SUMMATION
        ;

    #endif // ENABLE_PSI_CNN

    // py::class_<PsiDeepSigned>(m, "PsiDeepSigned")
    //     .def(py::init<
    //         const PsiDeepT<double, PsiDeepSigned::symmetric>&,
    //         const PsiDeepT<double, PsiDeepSigned::symmetric>&
    //     >())
    //     .def("copy", &PsiDeepSigned::copy)
    //     .def("__pos__", &PsiDeepSigned::copy)
    //     .def_readwrite("num_sites", &PsiDeepSigned::num_sites)
    //     .def_property(
    //         "log_prefactor",
    //         [](const PsiDeepSigned& psi) {return cuda_complex::to_std(psi.log_prefactor);},
    //         [](PsiDeepSigned& psi, typename std_dtype<PsiDeepSigned::dtype>::type value) {psi.log_prefactor = PsiDeepSigned::dtype(value);}
    //     )
    //     .def_readonly("plus", &PsiDeepSigned::psi_plus)
    //     .def_readonly("minus", &PsiDeepSigned::psi_minus)
    //     .def_readonly("gpu", &PsiDeepSigned::gpu)
    //     .def_readonly("N", &PsiDeepSigned::N)
    //     .def_readonly("num_params", &PsiDeepSigned::num_params)
    //     .def_property(
    //         "params",
    //         [](const PsiDeepSigned& psi) {return psi.get_params().to_pytensor_1d();},
    //         [](PsiDeepSigned& psi, const real_tensor<1u>& new_params) {
    //             psi.set_params(Array<double>(new_params, false));
    //         }
    //     )
    //     .def_property_readonly("symmetric", [](const PsiDeepSigned& psi){return psi.is_symmetric();})
    //     #ifdef ENABLE_EXACT_SUMMATION
    //     //! begin template
    //     .def("vector", [](PsiDeepSigned& psi, ExactSummation_t<Basis>& exact_summation) {return psi_vector(psi, exact_summation).to_pytensor_1d();})
    //     .def("norm", [](PsiDeepSigned& psi, ExactSummation_t<Basis>& exact_summation) {return psi_norm(psi, exact_summation);})
    //     //! end template
    //     #endif // ENABLE_EXACT_SUMMATION
    //     ;

    //! begin template
    py::class_<ClPsi>(m, "ClPsi{name}")
        .def(py::init<
            const unsigned int,
            const vector<Operator_t>&,
            const vector<unsigned int>&,
            const vector<unsigned int>&,
            const complex_tensor<1u>&,
            const typename ClPsi::PsiRef&,
            const double,
            const bool
        >())
        .def("copy", &ClPsi::copy)
        .def("__pos__", &ClPsi::copy)
        .def_readwrite("num_sites", &ClPsi::num_sites)
        .def_readonly("H_local", &ClPsi::H_local)
        .def_property(
            "log_prefactor",
            [](const ClPsi& psi) {return psi.log_prefactor.to_std();},
            [](ClPsi& psi, complex<double> value) {psi.log_prefactor = complex_t(value);}
        )
        .def_property(
            "ref_log_prefactor",
            [](const ClPsi& psi) {return psi.kernel().psi_ref.log_prefactor.to_std();},
            [](ClPsi& psi, complex<double> value) {
                psi.psi_ref.log_prefactor = complex_t(value);
                psi.update_psi_ref_kernel();
            }
        )
        // return just a copy, so writing has no effect..
        .def_property_readonly(
            "psi_ref",
            [](const ClPsi& psi) {return psi.psi_ref;}
        )
        .def_readonly("gpu", &ClPsi::gpu)
        .def_readonly("num_params", &ClPsi::num_params)
        .def_property_readonly("order", [](const ClPsi& psi) {return psi.get_order();})
        .def_property(
            "params",
            [](const ClPsi& psi) {return psi.params.to_pytensor_1d();},
            [](ClPsi& psi, const complex_tensor<1u>& new_params) {psi.params = new_params;}
        )
        // .def_property(
        //     "ref_params",
        //     [](const ClPsi& psi) {return psi.psi_ref.get_params().to_pytensor_1d();},
        //     [](ClPsi& psi, const complex_tensor<1u>& new_params) {
        //         psi.psi_ref.set_params(Array<complex_t>(new_params, false));
        //     }
        // )
        .def("update_psi_ref_kernel", &ClPsi::update_psi_ref_kernel)
        #ifdef ENABLE_EXACT_SUMMATION
        #ifdef ENABLE_SPINS
        .def("vector", [](ClPsi& psi, ExactSummation_t<Spins>& exact_summation) {return psi_vector(psi, exact_summation).to_pytensor_1d();})
        .def("norm", [](ClPsi& psi, ExactSummation_t<Spins>& exact_summation) {return psi_norm(psi, exact_summation);})
        .def("normalize", [](ClPsi& psi, ExactSummation_t<Spins>& exact_summation) {psi.log_prefactor -= log(psi_norm(psi, exact_summation));})
        #endif // ENABLE_SPINS
        #ifdef ENABLE_PAULIS
        .def("vector", [](ClPsi& psi, ExactSummation_t<PauliString>& exact_summation) {return psi_vector(psi, exact_summation).to_pytensor_1d();})
        .def("norm", [](ClPsi& psi, ExactSummation_t<PauliString>& exact_summation) {return psi_norm(psi, exact_summation);})
        .def("normalize", [](ClPsi& psi, ExactSummation_t<PauliString>& exact_summation) {psi.log_prefactor -= log(psi_norm(psi, exact_summation));})
        #endif // ENABLE_PAULIS
        #endif // ENABLE_EXACT_SUMMATION

        #ifdef ENABLE_MONTE_CARLO
        #ifdef ENABLE_SPINS
        .def("calibrate", [](ClPsi& psi, MonteCarloSpins& ensemble) {
            psi.psi_ref.log_prefactor -= complex_t(log_psi(psi.psi_ref, ensemble));
            psi.update_psi_ref_kernel();

            psi.log_prefactor -= complex_t(log_psi(psi, ensemble));
        })
        #endif // ENABLE_SPINS
        #endif // ENABLE_MONTE_CARLO

        ;
    //! end template

#ifdef ENABLE_PSI_CLASSICAL
    py::class_<PsiFullyPolarized>(m, "PsiFullyPolarized")
        .def(py::init<unsigned int, double>())
        .def_readwrite("num_sites", &PsiFullyPolarized::num_sites);
#endif // ENABLE_PSI_CLASSICAL

#ifdef ENABLE_PSI_EXACT
    py::class_<PsiExact>(m, "PsiExact")
        .def(py::init<real_tensor<1u>, bool>())
        .def_readwrite("num_sites", &PsiExact::num_sites);
#endif // ENABLE_PSI_EXACT


#ifdef USE_SUPER_OPERATOR

    py::class_<SparseMatrix>(m, "SparseMatrix")
        .def(py::init<
            bool,
            unsigned,
            unsigned,
            const vector<double>&,
            const vector<unsigned int>&
        >());

    py::class_<SuperOperator>(m, "SuperOperator")
        .def(py::init<
            const vector<SparseMatrix>&,
            const bool
        >());

#else

    py::class_<Operator>(m, "Operator")
        .def(py::init<
            const quantum_expression::PauliExpression&,
            const bool
        >())
        .def_property_readonly("expr", &Operator::to_expr)
        .def_property_readonly("expr_list", &Operator::to_expr_list)
        .def_readonly("num_strings", &Operator::num_strings);

#endif // USE_SUPER_OPERATOR



#ifdef ENABLE_SPINS
    py::class_<ann_on_gpu::Spins>(m, "Spins")
        .def(py::init<ann_on_gpu::Spins::dtype, const unsigned int>())
        .def_static("enumerate", ann_on_gpu::Spins::enumerate)
        .def("roll", &ann_on_gpu::Spins::roll)
        .def("array", &ann_on_gpu::Spins::array);
#endif // ENABLE_SPINS

#ifdef ENABLE_PAULIS
    py::class_<ann_on_gpu::PauliString>(m, "PauliString")
        .def(py::init<ann_on_gpu::PauliString::dtype, ann_on_gpu::PauliString::dtype>())
        .def_static("enumerate", ann_on_gpu::PauliString::enumerate)
        .def("roll", &ann_on_gpu::PauliString::roll)
        .def("array", &ann_on_gpu::PauliString::array);
#endif // ENABLE_PAULIS


#ifdef ENABLE_MONTE_CARLO
#ifdef ENABLE_SPINS
    py::class_<MonteCarloSpins>(m, "MonteCarloSpins")
        .def(py::init(&make_MonteCarloSpins))
        .def(py::init<MonteCarloSpins&>())
        // .def("set_total_z_symmetry", &MonteCarloSpins::set_total_z_symmetry)
        .def_property_readonly("gpu", [](const MonteCarloSpins& ensemble) {return ensemble.gpu;})
        .def_property_readonly("num_steps", &MonteCarloSpins::get_num_steps)
        .def_property_readonly("acceptance_rate", [](const MonteCarloSpins& mc){
            return float(mc.acceptances_ar.front()) / float(mc.acceptances_ar.front() + mc.rejections_ar.front());
        });
#endif // ENABLE_SPINS
#ifdef ENABLE_PAULIS
    py::class_<MonteCarloPaulis>(m, "MonteCarloPaulis")
        .def(py::init(&make_MonteCarloPaulis))
        .def(py::init<MonteCarloPaulis&>())
        // .def("set_total_z_symmetry", &MonteCarloPaulis::set_total_z_symmetry)
        .def_property_readonly("gpu", [](const MonteCarloPaulis& ensemble) {return ensemble.gpu;})
        .def_property_readonly("num_steps", &MonteCarloPaulis::get_num_steps)
        .def_property_readonly("acceptance_rate", [](const MonteCarloPaulis& mc){
            return float(mc.acceptances_ar.front()) / float(mc.acceptances_ar.front() + mc.rejections_ar.front());
        });
#endif // ENABLE_PAULIS
#endif // ENABLE_MONTE_CARLO


#ifdef ENABLE_EXACT_SUMMATION
#ifdef ENABLE_SPINS
    py::class_<ExactSummationSpins>(m, "ExactSummationSpins")
        .def(py::init<unsigned int, bool>())
        .def_property_readonly("gpu", [](const ExactSummationSpins& ensemble) {return ensemble.gpu;})
        // .def("set_total_z_symmetry", &ExactSummationSpins::set_total_z_symmetry)
        .def_property_readonly("num_steps", &ExactSummationSpins::get_num_steps);
#endif // ENABLE_SPINS
#ifdef ENABLE_PAULIS
    py::class_<ExactSummationPaulis>(m, "ExactSummationPaulis")
        .def(py::init<unsigned int, bool>())
        .def_property_readonly("gpu", [](const ExactSummationPaulis& ensemble) {return ensemble.gpu;})
        // .def("set_total_z_symmetry", &ExactSummationPaulis::set_total_z_symmetry)
        .def_property_readonly("num_steps", &ExactSummationPaulis::get_num_steps);
#endif // ENABLE_PAULIS
#endif // ENABLE_EXACT_SUMMATION


#ifndef LEAN_AND_MEAN

    py::class_<ExpectationValue>(m, "ExpectationValue")
        .def(py::init<bool>())
        //! begin template
        .def("__call__", &ExpectationValue::__call__<AllPsi, Ensemble<Basis>>)
        .def("__call__array", &ExpectationValue::__call__<AllPsi, Ensemble<Basis>>)
        .def("fluctuation", &ExpectationValue::fluctuation<AllPsi, Ensemble<Basis>>)
        .def("gradient", &ExpectationValue::gradient_py<AllPsi, Ensemble<Basis>>)
        .def("gradient_with_noise", &ExpectationValue::gradient<AllPsi, Ensemble<Basis>>)
        //! end template
        //! begin template
        .def("__call__", &ExpectationValue::__call__<ClPsi, FFPsi, Ensemble<Basis>>)
        //! end template
    ;


    py::class_<HilbertSpaceDistance>(m, "HilbertSpaceDistance")
        .def(py::init<unsigned int, bool>())
        //! begin template
        .def("__call__", &HilbertSpaceDistance::distance<FFPsi, FFPsi, Ensemble<Basis>>, "psi"_a, "psi_prime"_a, "operator_"_a, "is_unitary"_a, "spin_ensemble"_a)
        .def("gradient", &HilbertSpaceDistance::gradient_py<FFPsi, FFPsi, Ensemble<Basis>>, "psi"_a, "psi_prime"_a, "operator_"_a, "is_unitary"_a, "spin_ensemble"_a, "nu"_a)
        //! end template
    ;


    py::class_<KullbackLeibler>(m, "KullbackLeibler")
        .def(py::init<unsigned int, bool>())
        //! begin template
        .def("__call__", &KullbackLeibler::value<ClPsi, FFPsi, Ensemble<Basis>>)
        .def("gradient", &KullbackLeibler::gradient_py<ClPsi, FFPsi, Ensemble<Basis>>)
        .def("gradient_with_noise", [](KullbackLeibler& kl, ClPsi& psi, FFPsi& psi_prime, Ensemble<Basis>& ensemble, const double nu, double threshold){
            const auto result = kl.gradient_with_noise(psi, psi_prime, ensemble, nu, threshold);
            return make_tuple(
                get<0>(result).to_pytensor_1d(),
                get<1>(result).to_pytensor_1d(),
                get<2>(result)
            );
        })
        //! end template
        .def_property_readonly("total_weight", [](const KullbackLeibler& kl){return kl.total_weight.front();})
        .def_property_readonly("mean_deviation", [](const KullbackLeibler& kl){return kl.mean_deviation.front().to_std();})
        .def_readwrite("log_psi_scale", &KullbackLeibler::log_psi_scale)
    ;


    py::class_<TDVP>(m, "TDVP")
        .def(py::init<unsigned int, bool>())
        .def_property_readonly("S_matrix", [](const TDVP& tdvp){return tdvp.S_matrix.to_pytensor_2d({tdvp.num_params, tdvp.num_params});})
        .def_property_readonly("F_vector", [](const TDVP& tdvp){return tdvp.F_vector.to_pytensor_1d();})
        .def_property_readonly("O_k_vector", [](const TDVP& tdvp){return tdvp.O_k_ar.to_pytensor_1d();})
        .def_property_readonly("var_H", &TDVP::var_H)
        .def_readwrite("threshold", &TDVP::threshold)
        .def_property_readonly("total_weight", [](const TDVP& tdvp){return tdvp.total_weight.front();})
        //! begin template
        .def("eval_with_psi_ref", &TDVP::eval_with_psi_ref_py<ClPsi, Ensemble<Basis>>)
        //! end template
        //! begin template
        .def("eval", &TDVP::eval_py<AllPsi, Ensemble<Basis>>)
        //! end template
        ;

#endif // LEAN_AND_MEAN


    //! begin template
    m.def("log_psi_s", [](AllPsi& psi, const Basis& basis) {
        return log_psi_s(psi, basis);
    });
    m.def("psi_O_k", [](AllPsi& psi, const Basis& basis) {
        return psi_O_k(psi, basis).to_pytensor_1d();
    });
    //! end template

    #ifdef ENABLE_EXACT_SUMMATION
    //! begin template
    m.def("psi_O_k_vector", [](AllPsi& psi, ExactSummation_t<Basis>& ensemble) {
        return psi_O_k_vector(psi, ensemble).to_pytensor_1d();
    });
    //! end template
    #endif // ENABLE_EXACT_SUMMATION

    //! begin template
    m.def("log_psi", [](AllPsi& psi, Ensemble<Basis>& ensemble) {
        return log_psi(psi, ensemble);
    });
    m.def("psi_vector", [](AllPsi& psi, Ensemble<Basis>& ensemble) {
        return psi_vector(psi, ensemble).to_pytensor_1d();
    });
    m.def("log_psi_vector", [](AllPsi& psi, Ensemble<Basis>& ensemble) {
        return log_psi_vector(psi, ensemble).to_pytensor_1d();
    });
    //! end template

    //! begin template
    m.def("apply_operator", [](AllPsi& psi, const Operator_t& op, Ensemble<Basis>& ensemble){
        return apply_operator(psi, op, ensemble).to_pytensor_1d();
    });
    //! end template


    py::class_<RNGStates>(m, "RNGStates")
        .def(py::init<unsigned int, bool>());


    m.def("activation_function", [](const complex<double>& x, const unsigned int layer=0u) {
        return my_logcosh(complex_t(x.real(), x.imag()), layer).to_std();
    });

    m.def("setDevice", setDevice);
    m.def("start_profiling", start_profiling);
    m.def("stop_profiling", stop_profiling);
}
